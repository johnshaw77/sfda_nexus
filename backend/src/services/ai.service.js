/**
 * AIæœå‹™
 * æ•´åˆ Ollama å’Œ Gemini ç­‰ AI æ¨¡å‹æœå‹™
 */

import axios from "axios";
import { appConfig } from "../config/app.config.js";
import logger from "../utils/logger.util.js";

export class AIService {
  /**
   * èª¿ç”¨ Ollama æ¨¡å‹
   * @param {Object} options - èª¿ç”¨é¸é …
   * @returns {Promise<Object>} AIå›æ‡‰çµæœ
   */
  static async callOllama(options) {
    const {
      model = "qwen3:30b",
      messages = [],
      temperature = 0.7,
      max_tokens = 4096,
      stream = false,
      endpoint_url,
      enable_thinking = true, // æ–°å¢ï¼šå•Ÿç”¨æ€è€ƒæ¨¡å¼åƒæ•¸
    } = options;

    try {
      const ollamaEndpoint =
        endpoint_url || process.env.OLLAMA_ENDPOINT || "http://localhost:11434";

      // æª¢æŸ¥æ˜¯å¦æœ‰å¤šæ¨¡æ…‹å…§å®¹
      const hasMultimodal = messages.some((msg) => Array.isArray(msg.content));

      console.log("=== OLLAMA èª¿ç”¨é–‹å§‹ ===");
      console.log("æ¨¡å‹:", model);
      console.log("ç«¯é»URL:", ollamaEndpoint);
      console.log("æ¶ˆæ¯æ•¸é‡:", messages.length);
      console.log("å¤šæ¨¡æ…‹å…§å®¹:", hasMultimodal ? "æ˜¯" : "å¦");
      console.log("ä½¿ç”¨è³‡æ–™åº«é…ç½®çš„ç«¯é»:", endpoint_url ? "æ˜¯" : "å¦");

      if (hasMultimodal) {
        console.log("=== å¤šæ¨¡æ…‹æ¶ˆæ¯è©³æƒ… ===");
        messages.forEach((msg, index) => {
          if (Array.isArray(msg.content)) {
            console.log(`æ¶ˆæ¯ ${index + 1} (${msg.role}):`);
            msg.content.forEach((part, partIndex) => {
              if (part.type === "text") {
                console.log(
                  `  æ–‡å­—éƒ¨åˆ† ${partIndex + 1}: "${(part.text || "").substring(0, 50)}..."`
                );
              } else if (part.type === "image_url") {
                console.log(
                  `  åœ–ç‰‡éƒ¨åˆ† ${partIndex + 1}: ${(part.image_url?.url || "").substring(0, 50)}...`
                );
              }
            });
          } else {
            console.log(`æ¶ˆæ¯ ${index + 1} (${msg.role}): ç´”æ–‡å­—`);
          }
        });
      }

      // è½‰æ›å¤šæ¨¡æ…‹æ¶ˆæ¯ç‚º Ollama åŸç”Ÿæ ¼å¼
      const ollamaMessages = hasMultimodal
        ? this.convertToOllamaFormat(messages)
        : messages;

      // æª¢æŸ¥æ˜¯å¦ç‚ºæ”¯æ´æ€è€ƒæ¨¡å¼çš„æ¨¡å‹
      const isThinkingModel =
        model.toLowerCase().includes("qwen3") ||
        model.toLowerCase().includes("deepseek-r1") ||
        model.toLowerCase().includes("smallthinker");

      const requestData = {
        model: model,
        messages: ollamaMessages,
        options: {
          temperature: temperature,
          num_predict: max_tokens,
        },
        stream: stream,
      };

      // ç‚ºæ”¯æ´æ€è€ƒæ¨¡å¼çš„æ¨¡å‹æ·»åŠ  think åƒæ•¸
      if (isThinkingModel && enable_thinking) {
        requestData.think = true;
        console.log("=== å•Ÿç”¨æ€è€ƒæ¨¡å¼ ===");
        console.log("æ¨¡å‹æ”¯æ´æ€è€ƒæ¨¡å¼:", model);
        console.log("æ€è€ƒæ¨¡å¼å·²å•Ÿç”¨");
      }

      logger.debug("èª¿ç”¨ Ollama æ¨¡å‹", {
        model,
        messageCount: messages.length,
        temperature,
        max_tokens,
        hasMultimodal,
      });

      const startTime = Date.now();
      const response = await axios.post(
        `${ollamaEndpoint}/api/chat`,
        requestData,
        {
          timeout: 300000, // 5åˆ†é˜è¶…æ™‚
          headers: {
            "Content-Type": "application/json",
          },
        }
      );

      const processingTime = Date.now() - startTime;

      const messageContent = response.data.message?.content || "";
      const thinkingContent = response.data.message?.thinking || null;

      const result = {
        content: messageContent,
        thinking_content: thinkingContent, // æ–°å¢ï¼šæ€è€ƒå…§å®¹
        model: model,
        tokens_used: this.estimateTokens(messageContent),
        cost: 0, // Ollama æœ¬åœ°æ¨¡å‹ç„¡è²»ç”¨
        processing_time: processingTime,
        provider: "ollama",
        model_info: {
          model: model,
          created_at: response.data.created_at,
          done: response.data.done,
          total_duration: response.data.total_duration,
          load_duration: response.data.load_duration,
          prompt_eval_duration: response.data.prompt_eval_duration,
          eval_duration: response.data.eval_duration,
          prompt_eval_count: response.data.prompt_eval_count,
          eval_count: response.data.eval_count,
        },
      };

      // èª¿è©¦ï¼šæ‰“å°å®Œæ•´çš„ AI å›æ‡‰ä¿¡æ¯
      console.log("=== OLLAMA AI å›æ‡‰èª¿è©¦ä¿¡æ¯ ===");
      console.log("å¤šæ¨¡æ…‹è«‹æ±‚:", hasMultimodal ? "æ˜¯" : "å¦");
      console.log(
        "æ€è€ƒæ¨¡å¼å•Ÿç”¨:",
        isThinkingModel && enable_thinking ? "æ˜¯" : "å¦"
      );
      console.log("åŸå§‹å›æ‡‰æ•¸æ“š:", JSON.stringify(response.data, null, 2));
      console.log("è§£æå¾Œçš„å›æ‡‰å…§å®¹:", result.content);
      console.log("å›æ‡‰å…§å®¹é•·åº¦:", result.content.length);
      if (result.thinking_content) {
        console.log("=== æ€è€ƒå…§å®¹ ===");
        console.log("æ€è€ƒå…§å®¹é•·åº¦:", result.thinking_content.length);
        console.log(
          "æ€è€ƒå…§å®¹é è¦½:",
          result.thinking_content.substring(0, 200) + "..."
        );
      }
      console.log("ä¼°ç®— tokens:", result.tokens_used);
      console.log("è™•ç†æ™‚é–“:", processingTime, "ms");

      if (hasMultimodal) {
        console.log("=== å¤šæ¨¡æ…‹è™•ç†çµæœ ===");
        if (
          result.content.includes("åœ–") ||
          result.content.includes("image") ||
          result.content.includes("çœ‹åˆ°")
        ) {
          console.log("âœ… AI å›æ‡‰ä¼¼ä¹åŒ…å«å°åœ–ç‰‡çš„æè¿°");
        } else {
          console.log("âš ï¸  AI å›æ‡‰å¯èƒ½æ²’æœ‰è™•ç†åœ–ç‰‡å…§å®¹");
        }
      }

      console.log("=== AI å›æ‡‰èª¿è©¦ä¿¡æ¯çµæŸ ===\n");

      logger.info("Ollama èª¿ç”¨æˆåŠŸ", {
        model,
        tokens: result.tokens_used,
        time: processingTime,
        contentLength: result.content.length,
        hasMultimodal,
      });

      return result;
    } catch (error) {
      logger.error("Ollama èª¿ç”¨å¤±æ•—", {
        model,
        error: error.message,
        endpoint: process.env.OLLAMA_ENDPOINT,
      });

      console.error("=== OLLAMA èª¿ç”¨å¤±æ•—è©³æƒ… ===");
      console.error("éŒ¯èª¤æ¶ˆæ¯:", error.message);
      if (error.response) {
        console.error("HTTP ç‹€æ…‹:", error.response.status);
        console.error("éŸ¿æ‡‰æ•¸æ“š:", error.response.data);
      }
      console.error("=== éŒ¯èª¤è©³æƒ…çµæŸ ===");

      throw new Error(`Ollama èª¿ç”¨å¤±æ•—: ${error.message}`);
    }
  }

  /**
   * èª¿ç”¨ Google Gemini æ¨¡å‹
   * @param {Object} options - èª¿ç”¨é¸é …
   * @returns {Promise<Object>} AIå›æ‡‰çµæœ
   */
  static async callGemini(options) {
    const {
      model = "gemini-2.0-flash",
      messages = [],
      temperature = 0.7,
      max_tokens = 8192,
      api_key,
    } = options;

    try {
      const apiKey = api_key || process.env.GEMINI_API_KEY;
      if (!apiKey) {
        throw new Error("Gemini API Key æœªé…ç½®");
      }

      // è½‰æ›è¨Šæ¯æ ¼å¼ç‚º Gemini æ ¼å¼
      const geminiMessages = this.convertToGeminiFormat(messages);

      const requestData = {
        contents: geminiMessages,
        generationConfig: {
          temperature: temperature,
          maxOutputTokens: max_tokens,
          topP: 0.9,
          topK: 40,
        },
        safetySettings: [
          {
            category: "HARM_CATEGORY_HARASSMENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE",
          },
          {
            category: "HARM_CATEGORY_HATE_SPEECH",
            threshold: "BLOCK_MEDIUM_AND_ABOVE",
          },
          {
            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE",
          },
          {
            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE",
          },
        ],
      };

      console.log("=== GEMINI èª¿ç”¨é–‹å§‹ ===");
      console.log("æ¨¡å‹:", model);
      console.log("APIå¯†é‘°ä¾†æº:", api_key ? "è³‡æ–™åº«" : "ç’°å¢ƒè®Šæ•¸");
      console.log("æ¶ˆæ¯æ•¸é‡:", messages.length);
      console.log("æº«åº¦:", temperature);
      console.log("æœ€å¤§tokens:", max_tokens);

      logger.debug("èª¿ç”¨ Gemini æ¨¡å‹", {
        model,
        messageCount: messages.length,
        temperature,
        max_tokens,
      });

      const startTime = Date.now();
      const response = await axios.post(
        `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
        requestData,
        {
          timeout: 60000, // 1åˆ†é˜è¶…æ™‚
          headers: {
            "Content-Type": "application/json",
          },
        }
      );

      const processingTime = Date.now() - startTime;
      const candidate = response.data.candidates?.[0];
      const content = candidate?.content?.parts?.[0]?.text || "";

      // è¨ˆç®—è²»ç”¨ï¼ˆåŸºæ–¼ token æ•¸é‡çš„ä¼°ç®—ï¼‰
      const inputTokens = this.estimateTokens(
        messages.map((m) => m.content).join(" ")
      );
      const outputTokens = this.estimateTokens(content);
      const cost = this.calculateGeminiCost(inputTokens, outputTokens, model);

      const result = {
        content: content,
        model: model,
        tokens_used: inputTokens + outputTokens,
        cost: cost,
        processing_time: processingTime,
        provider: "gemini",
        model_info: {
          model: model,
          finish_reason: candidate?.finishReason,
          safety_ratings: candidate?.safetyRatings,
          usage_metadata: response.data.usageMetadata,
        },
      };

      // èª¿è©¦ï¼šæ‰“å°å®Œæ•´çš„ AI å›æ‡‰ä¿¡æ¯
      console.log("=== GEMINI AI å›æ‡‰èª¿è©¦ä¿¡æ¯ ===");
      console.log("åŸå§‹å›æ‡‰æ•¸æ“š:", JSON.stringify(response.data, null, 2));
      console.log("è§£æå¾Œçš„å›æ‡‰å…§å®¹:", result.content);
      console.log("å›æ‡‰å…§å®¹é•·åº¦:", result.content.length);
      console.log("è¼¸å…¥ tokens:", inputTokens);
      console.log("è¼¸å‡º tokens:", outputTokens);
      console.log("ç¸½è²»ç”¨:", cost);
      console.log("è™•ç†æ™‚é–“:", processingTime, "ms");
      console.log("=== AI å›æ‡‰èª¿è©¦ä¿¡æ¯çµæŸ ===\n");

      logger.info("Gemini èª¿ç”¨æˆåŠŸ", {
        model,
        inputTokens,
        outputTokens,
        cost,
        time: processingTime,
        contentLength: content.length,
      });

      return result;
    } catch (error) {
      logger.error("Gemini èª¿ç”¨å¤±æ•—", {
        model,
        error: error.message,
        status: error.response?.status,
        data: error.response?.data,
      });
      throw new Error(`Gemini èª¿ç”¨å¤±æ•—: ${error.message}`);
    }
  }

  /**
   * èª¿ç”¨AIæ¨¡å‹ï¼ˆæ”¯æ´ä¸²æµæ¨¡å¼ï¼‰
   * @param {Object} options - AIèª¿ç”¨é¸é …
   * @param {string} options.provider - AIæä¾›å•† (ollama, gemini)
   * @param {string} options.model - æ¨¡å‹åç¨±
   * @param {Array} options.messages - æ¶ˆæ¯é™£åˆ—
   * @param {number} [options.temperature=0.7] - æº«åº¦åƒæ•¸
   * @param {number} [options.max_tokens=4096] - æœ€å¤§tokenæ•¸
   * @param {boolean} [options.stream=false] - æ˜¯å¦ä½¿ç”¨ä¸²æµæ¨¡å¼
   * @param {string} [options.endpoint_url] - è‡ªå®šç¾©ç«¯é»URL
   * @param {string} [options.api_key] - è‡ªå®šç¾©APIå¯†é‘°
   * @returns {Promise<Object|AsyncGenerator>} AIå›æ‡‰æˆ–ä¸²æµç”Ÿæˆå™¨
   */
  static async callModel(options) {
    const {
      provider,
      model,
      messages,
      temperature = 0.7,
      max_tokens = 4096,
      stream = false,
      endpoint_url,
      api_key,
    } = options;

    logger.info("AIæ¨¡å‹èª¿ç”¨é–‹å§‹", {
      provider,
      model,
      messageCount: messages.length,
      temperature,
      max_tokens,
      stream,
    });

    const startTime = Date.now();

    try {
      switch (provider) {
        case "ollama":
          if (stream) {
            return await this.callOllamaStream(
              model,
              messages,
              temperature,
              max_tokens,
              endpoint_url
            );
          } else {
            return await this.callOllama({
              model,
              messages,
              temperature,
              max_tokens,
              stream,
              endpoint_url,
            });
          }

        case "gemini":
          if (stream) {
            return await this.callGeminiStream(
              model,
              messages,
              temperature,
              max_tokens,
              api_key
            );
          } else {
            return await this.callGemini({
              model,
              messages,
              temperature,
              max_tokens,
              api_key,
            });
          }

        default:
          throw new Error(`ä¸æ”¯æŒçš„AIæä¾›å•†: ${provider}`);
      }
    } catch (error) {
      logger.error("AIæ¨¡å‹èª¿ç”¨å¤±æ•—", {
        provider,
        model,
        error: error.message,
        processingTime: Date.now() - startTime,
      });
      throw error;
    }
  }

  /**
   * èª¿ç”¨Ollamaæ¨¡å‹ï¼ˆä¸²æµæ¨¡å¼ï¼‰
   */
  static async callOllamaStream(
    model,
    messages,
    temperature,
    max_tokens,
    endpoint_url
  ) {
    const startTime = Date.now();

    try {
      const ollamaUrl =
        endpoint_url || process.env.OLLAMA_ENDPOINT || "http://localhost:11434";

      // æª¢æŸ¥æ˜¯å¦æœ‰å¤šæ¨¡æ…‹å…§å®¹
      const hasMultimodal = messages.some((msg) => Array.isArray(msg.content));

      // æª¢æŸ¥æ˜¯å¦ç‚ºæ”¯æŒæ€è€ƒæ¨¡å¼çš„æ¨¡å‹
      const isThinkingModel = this.isThinkingCapableModel(model);

      console.log("=== OLLAMA ä¸²æµèª¿ç”¨é–‹å§‹ ===");
      console.log("URL:", `${ollamaUrl}/api/chat`);
      console.log("æ¨¡å‹:", model);
      console.log("ç«¯é»URL:", ollamaUrl);
      console.log("æ¶ˆæ¯æ•¸é‡:", messages.length);
      console.log("å¤šæ¨¡æ…‹å…§å®¹:", hasMultimodal ? "æ˜¯" : "å¦");
      console.log("æ€è€ƒæ¨¡å¼:", isThinkingModel ? "å•Ÿç”¨" : "ä¸æ”¯æŒ");
      console.log("ä¸²æµæ¨¡å¼: å•Ÿç”¨");
      console.log("ä½¿ç”¨è³‡æ–™åº«é…ç½®çš„ç«¯é»:", endpoint_url ? "æ˜¯" : "å¦");

      if (hasMultimodal) {
        console.log("=== ä¸²æµå¤šæ¨¡æ…‹æ¶ˆæ¯è©³æƒ… ===");
        messages.forEach((msg, index) => {
          if (Array.isArray(msg.content)) {
            console.log(`æ¶ˆæ¯ ${index + 1} (${msg.role}):`);
            msg.content.forEach((part, partIndex) => {
              if (part.type === "text") {
                console.log(
                  `  æ–‡å­—éƒ¨åˆ† ${partIndex + 1}: "${(part.text || "").substring(0, 50)}..."`
                );
              } else if (part.type === "image_url") {
                console.log(
                  `  åœ–ç‰‡éƒ¨åˆ† ${partIndex + 1}: "${(part.image_url?.url || "").substring(0, 50)}..."`
                );
              }
            });
          } else {
            console.log(`æ¶ˆæ¯ ${index + 1} (${msg.role}): ç´”æ–‡å­—`);
          }
        });
      }

      // æ§‹å»ºè«‹æ±‚æ•¸æ“š
      const requestData = {
        model: model,
        messages: hasMultimodal
          ? this.convertToOllamaFormat(messages)
          : messages,
        stream: true, // å•Ÿç”¨ä¸²æµ
        options: {
          temperature: temperature,
          num_predict: max_tokens,
        },
      };

      // ç‚ºæ”¯æŒæ€è€ƒæ¨¡å¼çš„æ¨¡å‹æ·»åŠ  think åƒæ•¸
      if (isThinkingModel) {
        requestData.think = true;
        console.log("=== æ€è€ƒæ¨¡å¼å·²å•Ÿç”¨ ===");
      }

      const response = await fetch(`${ollamaUrl}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestData),
      });

      if (!response.ok) {
        throw new Error(
          `Ollama APIéŒ¯èª¤: ${response.status} ${response.statusText}`
        );
      }

      console.log("=== OLLAMA ä¸²æµå›æ‡‰æº–å‚™å°±ç·’ ===");
      if (hasMultimodal) {
        console.log("æº–å‚™æ¥æ”¶å¤šæ¨¡æ…‹ä¸²æµå›æ‡‰...");
      }
      if (isThinkingModel) {
        console.log("æº–å‚™æ¥æ”¶æ€è€ƒå…§å®¹...");
      }

      // è¿”å›ä¸²æµç”Ÿæˆå™¨
      return this.createOllamaStreamGenerator(
        response,
        startTime,
        hasMultimodal,
        isThinkingModel
      );
    } catch (error) {
      console.error("=== OLLAMA ä¸²æµèª¿ç”¨å¤±æ•— ===");
      console.error("éŒ¯èª¤:", error.message);
      throw new Error(`Ollamaä¸²æµèª¿ç”¨å¤±æ•—: ${error.message}`);
    }
  }

  /**
   * å‰µå»ºOllamaä¸²æµç”Ÿæˆå™¨
   */
  static async *createOllamaStreamGenerator(
    response,
    startTime,
    hasMultimodal,
    isThinkingModel
  ) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = "";
    let totalTokens = 0;
    let fullContent = "";
    let thinkingContent = "";
    let isInThinkingMode = false;
    let currentThinkingBuffer = "";

    try {
      while (true) {
        const { done, value } = await reader.read();

        if (done) {
          break;
        }

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");

        // ä¿ç•™æœ€å¾Œä¸€å€‹å¯èƒ½ä¸å®Œæ•´çš„è¡Œ
        buffer = lines.pop() || "";

        for (const line of lines) {
          if (line.trim()) {
            try {
              const data = JSON.parse(line);

              // è™•ç† DeepSeek-R1 çš„æ€è€ƒå…§å®¹ï¼ˆç›´æ¥å¾ message.thinkingï¼‰
              if (isThinkingModel && data.message && data.message.thinking) {
                thinkingContent += data.message.thinking;
                console.log(
                  "=== ä¸²æµæ€è€ƒå…§å®¹ ===",
                  data.message.thinking.substring(0, 100) + "..."
                );
              }

              if (data.message && data.message.content) {
                const content = data.message.content;
                let processedContent = content;
                let currentThinking = "";

                // è™•ç† Qwen3 çš„ <think> æ¨™ç±¤
                if (isThinkingModel) {
                  const thinkStart = content.indexOf("<think>");
                  const thinkEnd = content.indexOf("</think>");

                  if (thinkStart !== -1 && !isInThinkingMode) {
                    // ç™¼ç¾æ€è€ƒé–‹å§‹æ¨™ç±¤
                    isInThinkingMode = true;
                    console.log("ğŸ§  é–‹å§‹ Qwen3 æ€è€ƒæ¨¡å¼");

                    // æå– <think> ä¹‹å‰çš„å…§å®¹ä½œç‚ºæ­£å¸¸å›æ‡‰
                    processedContent = content.substring(0, thinkStart);

                    // å¦‚æœåŒä¸€å¡Šä¸­æœ‰çµæŸæ¨™ç±¤
                    if (thinkEnd !== -1) {
                      // å®Œæ•´çš„æ€è€ƒå¡Š
                      const thinkContent = content.substring(
                        thinkStart + 7,
                        thinkEnd
                      );
                      thinkingContent += thinkContent;
                      processedContent += content.substring(thinkEnd + 8);
                      isInThinkingMode = false;
                      console.log(
                        "ğŸ§  å®Œæ•´æ€è€ƒå¡Š:",
                        thinkContent.substring(0, 50) + "..."
                      );
                    } else {
                      // åªæœ‰é–‹å§‹æ¨™ç±¤ï¼Œé–‹å§‹æ”¶é›†
                      currentThinkingBuffer = content.substring(thinkStart + 7);
                    }
                  } else if (isInThinkingMode) {
                    // åœ¨æ€è€ƒæ¨¡å¼ä¸­
                    if (thinkEnd !== -1) {
                      // æ‰¾åˆ°çµæŸæ¨™ç±¤
                      currentThinkingBuffer += content.substring(0, thinkEnd);
                      thinkingContent += currentThinkingBuffer;
                      processedContent = content.substring(thinkEnd + 8);
                      isInThinkingMode = false;
                      currentThinkingBuffer = "";
                      console.log(
                        "ğŸ§  æ€è€ƒå…§å®¹å®Œæˆ:",
                        thinkingContent.substring(thinkingContent.length - 50)
                      );
                    } else {
                      // ä»åœ¨æ€è€ƒä¸­ï¼Œç¹¼çºŒæ”¶é›†
                      currentThinkingBuffer += content;
                      thinkingContent += content;
                      processedContent = "";
                      console.log(
                        "ğŸ§  æ”¶é›†æ€è€ƒå…§å®¹:",
                        content.substring(0, 20) + "..."
                      );
                    }
                  }
                }

                // åªæœ‰éæ€è€ƒå…§å®¹æ‰æ·»åŠ åˆ° fullContent
                if (processedContent) {
                  fullContent += processedContent;
                }
                totalTokens++;

                // ç”¢å‡ºä¸²æµæ•¸æ“šå¡Š
                yield {
                  type: "content",
                  content: processedContent, // ç™¼é€è™•ç†å¾Œçš„å…§å®¹ï¼ˆä¸åŒ…å« <think> æ¨™ç±¤ï¼‰
                  full_content: fullContent,
                  thinking_content: thinkingContent, // ç™¼é€ç´¯ç©çš„æ€è€ƒå…§å®¹
                  tokens_used: totalTokens,
                  done: data.done || false,
                  model: data.model,
                  provider: "ollama",
                };
              }

              if (data.done) {
                const processingTime = Date.now() - startTime;

                console.log("=== OLLAMA ä¸²æµå®Œæˆ ===");
                console.log("å¤šæ¨¡æ…‹è«‹æ±‚:", hasMultimodal ? "æ˜¯" : "å¦");
                console.log("æ€è€ƒæ¨¡å¼:", isThinkingModel ? "æ˜¯" : "å¦");
                console.log("ç¸½å…§å®¹é•·åº¦:", fullContent.length);
                console.log("æ€è€ƒå…§å®¹é•·åº¦:", thinkingContent.length);
                console.log("è™•ç†æ™‚é–“:", processingTime, "ms");
                console.log("ç¸½ tokens:", totalTokens);

                if (isThinkingModel && thinkingContent) {
                  console.log("=== ä¸²æµæ€è€ƒå…§å®¹æå–å®Œæˆ ===");
                  console.log(
                    "æ€è€ƒå…§å®¹é è¦½:",
                    thinkingContent.substring(0, 200) + "..."
                  );
                }

                if (hasMultimodal) {
                  console.log("=== ä¸²æµå¤šæ¨¡æ…‹è™•ç†çµæœ ===");
                  if (
                    fullContent.includes("åœ–") ||
                    fullContent.includes("image") ||
                    fullContent.includes("çœ‹åˆ°")
                  ) {
                    console.log("âœ… ä¸²æµAI å›æ‡‰ä¼¼ä¹åŒ…å«å°åœ–ç‰‡çš„æè¿°");
                  } else {
                    console.log("âš ï¸  ä¸²æµAI å›æ‡‰å¯èƒ½æ²’æœ‰è™•ç†åœ–ç‰‡å…§å®¹");
                  }
                }

                // ç”¢å‡ºæœ€çµ‚çµ±è¨ˆæ•¸æ“š
                yield {
                  type: "done",
                  full_content: fullContent,
                  thinking_content: thinkingContent,
                  tokens_used: totalTokens,
                  processing_time: processingTime,
                  cost: this.calculateCost("ollama", totalTokens),
                  model_info: data.model,
                  provider: "ollama",
                };
                break;
              }
            } catch (parseError) {
              console.warn(
                "Ollamaä¸²æµæ•¸æ“šè§£æéŒ¯èª¤:",
                parseError.message,
                "è¡Œ:",
                line
              );
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * èª¿ç”¨Geminiæ¨¡å‹ï¼ˆä¸²æµæ¨¡å¼ï¼‰
   */
  static async callGeminiStream(
    model,
    messages,
    temperature,
    max_tokens,
    api_key
  ) {
    const startTime = Date.now();

    try {
      const geminiApiKey = api_key || process.env.GEMINI_API_KEY;
      if (!geminiApiKey) {
        throw new Error("æœªé…ç½®Gemini API Key");
      }

      console.log("=== GEMINI ä¸²æµèª¿ç”¨é–‹å§‹ ===");
      console.log("æ¨¡å‹:", model);
      console.log("APIå¯†é‘°ä¾†æº:", api_key ? "è³‡æ–™åº«" : "ç’°å¢ƒè®Šæ•¸");
      console.log("æ¶ˆæ¯æ•¸é‡:", messages.length);
      console.log("ä¸²æµæ¨¡å¼: å•Ÿç”¨");

      // è½‰æ›æ¶ˆæ¯æ ¼å¼ç‚ºGeminiæ ¼å¼
      const geminiMessages = this.convertToGeminiFormat(messages);

      const requestBody = {
        contents: geminiMessages,
        generationConfig: {
          temperature: temperature,
          maxOutputTokens: max_tokens,
        },
      };

      console.log("Geminiè«‹æ±‚é«”:", JSON.stringify(requestBody, null, 2));

      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?key=${geminiApiKey}&alt=sse`,
        {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestBody),
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `Gemini APIéŒ¯èª¤: ${response.status} ${response.statusText} - ${errorText}`
        );
      }

      console.log("=== GEMINI ä¸²æµå›æ‡‰æº–å‚™å°±ç·’ ===");

      // è¿”å›ä¸²æµç”Ÿæˆå™¨
      return this.createGeminiStreamGenerator(response, startTime, model);
    } catch (error) {
      console.error("=== GEMINI ä¸²æµèª¿ç”¨å¤±æ•— ===");
      console.error("éŒ¯èª¤:", error.message);
      throw new Error(`Geminiä¸²æµèª¿ç”¨å¤±æ•—: ${error.message}`);
    }
  }

  /**
   * å‰µå»ºGeminiä¸²æµç”Ÿæˆå™¨
   */
  static async *createGeminiStreamGenerator(response, startTime, model) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = "";
    let totalTokens = 0;
    let fullContent = "";

    try {
      while (true) {
        const { done, value } = await reader.read();

        if (done) {
          break;
        }

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");

        // ä¿ç•™æœ€å¾Œä¸€å€‹å¯èƒ½ä¸å®Œæ•´çš„è¡Œ
        buffer = lines.pop() || "";

        for (const line of lines) {
          if (line.startsWith("data: ")) {
            const jsonStr = line.slice(6).trim();

            if (jsonStr === "[DONE]") {
              const processingTime = Date.now() - startTime;

              console.log("=== GEMINI ä¸²æµå®Œæˆ ===");
              console.log("ç¸½å…§å®¹é•·åº¦:", fullContent.length);
              console.log("è™•ç†æ™‚é–“:", processingTime, "ms");
              console.log("ç¸½ tokens:", totalTokens);

              // ç”¢å‡ºæœ€çµ‚çµ±è¨ˆæ•¸æ“š
              yield {
                type: "done",
                full_content: fullContent,
                tokens_used: totalTokens,
                processing_time: processingTime,
                cost: this.calculateCost("gemini", totalTokens),
                model_info: model,
                provider: "gemini",
              };
              return;
            }

            try {
              const data = JSON.parse(jsonStr);

              if (
                data.candidates &&
                data.candidates[0] &&
                data.candidates[0].content
              ) {
                const parts = data.candidates[0].content.parts;
                if (parts && parts[0] && parts[0].text) {
                  const content = parts[0].text;
                  fullContent += content;
                  totalTokens++;

                  // ç”¢å‡ºä¸²æµæ•¸æ“šå¡Š
                  yield {
                    type: "content",
                    content: content,
                    full_content: fullContent,
                    tokens_used: totalTokens,
                    done: false,
                    model: model,
                    provider: "gemini",
                  };
                }
              }

              // æª¢æŸ¥æ˜¯å¦æœ‰ä½¿ç”¨çµ±è¨ˆä¿¡æ¯
              if (data.usageMetadata) {
                totalTokens = data.usageMetadata.totalTokenCount || totalTokens;
              }

              // æª¢æŸ¥æ˜¯å¦å®Œæˆï¼ˆGeminiå¯èƒ½é€šéfinishReasonæ¨™ç¤ºå®Œæˆï¼‰
              if (
                data.candidates &&
                data.candidates[0] &&
                data.candidates[0].finishReason
              ) {
                const processingTime = Date.now() - startTime;

                console.log("=== GEMINI ä¸²æµå®Œæˆï¼ˆé€šéfinishReasonï¼‰ ===");
                console.log("å®ŒæˆåŸå› :", data.candidates[0].finishReason);
                console.log("ç¸½å…§å®¹é•·åº¦:", fullContent.length);
                console.log("è™•ç†æ™‚é–“:", processingTime, "ms");
                console.log("ç¸½ tokens:", totalTokens);

                // ç”¢å‡ºæœ€çµ‚çµ±è¨ˆæ•¸æ“š
                yield {
                  type: "done",
                  full_content: fullContent,
                  tokens_used: totalTokens,
                  processing_time: processingTime,
                  cost: this.calculateCost("gemini", totalTokens),
                  model_info: model,
                  provider: "gemini",
                };
                return;
              }
            } catch (parseError) {
              console.warn(
                "Geminiä¸²æµæ•¸æ“šè§£æéŒ¯èª¤:",
                parseError.message,
                "æ•¸æ“š:",
                jsonStr
              );
            }
          }
        }
      }

      // å¦‚æœå¾ªç’°çµæŸä½†æ²’æœ‰ç™¼é€å®Œæˆä¿¡è™Ÿï¼Œæ‰‹å‹•ç™¼é€
      if (fullContent) {
        const processingTime = Date.now() - startTime;

        console.log("=== GEMINI ä¸²æµå®Œæˆï¼ˆå¾ªç’°çµæŸï¼‰ ===");
        console.log("ç¸½å…§å®¹é•·åº¦:", fullContent.length);
        console.log("è™•ç†æ™‚é–“:", processingTime, "ms");
        console.log("ç¸½ tokens:", totalTokens);

        // ç”¢å‡ºæœ€çµ‚çµ±è¨ˆæ•¸æ“š
        yield {
          type: "done",
          full_content: fullContent,
          tokens_used: totalTokens,
          processing_time: processingTime,
          cost: this.calculateCost("gemini", totalTokens),
          model_info: model,
          provider: "gemini",
        };
      }
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * è½‰æ›å¤šæ¨¡æ…‹æ¶ˆæ¯ç‚º Ollama åŸç”Ÿæ ¼å¼
   * @param {Array} messages - æ¨™æº–è¨Šæ¯æ ¼å¼
   * @returns {Array} Ollama æ ¼å¼çš„è¨Šæ¯
   */
  static convertToOllamaFormat(messages) {
    console.log("=== è½‰æ›è¨Šæ¯ç‚º Ollama æ ¼å¼ ===");
    console.log("è¼¸å…¥è¨Šæ¯æ•¸é‡:", messages.length);

    const result = messages.map((msg, index) => {
      if (Array.isArray(msg.content)) {
        // è™•ç†å¤šæ¨¡æ…‹å…§å®¹
        let textContent = "";
        const images = [];

        console.log(
          `è™•ç†å¤šæ¨¡æ…‹è¨Šæ¯ ${index + 1}ï¼Œéƒ¨åˆ†æ•¸é‡:`,
          msg.content.length
        );

        msg.content.forEach((part, partIndex) => {
          console.log(`  éƒ¨åˆ† ${partIndex + 1}:`, part.type);
          if (part.type === "text") {
            textContent += part.text || "";
          } else if (part.type === "image_url") {
            // æå– base64 åœ–ç‰‡æ•¸æ“š
            // part.image_url å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–ç‰©ä»¶
            let imageUrl = "";
            if (typeof part.image_url === "string") {
              imageUrl = part.image_url;
            } else if (part.image_url?.url) {
              imageUrl = part.image_url.url;
            }

            const base64Data = imageUrl.replace(
              /^data:image\/[^;]+;base64,/,
              ""
            );
            images.push(base64Data);
            console.log(`  æ·»åŠ åœ–ç‰‡ï¼Œbase64é•·åº¦: ${base64Data.length}`);
          }
        });

        const ollamaMessage = {
          role: msg.role,
          content: textContent,
        };

        if (images.length > 0) {
          ollamaMessage.images = images;
        }

        console.log(
          `Ollamaè¨Šæ¯ ${index + 1}: æ–‡å­—é•·åº¦=${textContent.length}, åœ–ç‰‡æ•¸é‡=${images.length}`
        );
        return ollamaMessage;
      } else {
        // ç´”æ–‡å­—æ¶ˆæ¯ä¿æŒä¸è®Š
        console.log(
          `ç´”æ–‡å­—è¨Šæ¯ ${index + 1}: ${(msg.content || "").substring(0, 50)}...`
        );
        return msg;
      }
    });

    console.log("è½‰æ›å®Œæˆï¼ŒOllamaè¨Šæ¯æ•¸é‡:", result.length);
    return result;
  }

  /**
   * è½‰æ›è¨Šæ¯æ ¼å¼ç‚º Gemini æ ¼å¼
   * @param {Array} messages - æ¨™æº–è¨Šæ¯æ ¼å¼
   * @returns {Array} Gemini æ ¼å¼çš„è¨Šæ¯
   */
  static convertToGeminiFormat(messages) {
    console.log("=== è½‰æ›è¨Šæ¯ç‚º Gemini æ ¼å¼ ===");
    console.log("è¼¸å…¥è¨Šæ¯æ•¸é‡:", messages.length);

    const result = messages
      .filter((msg) => msg.role !== "system") // Gemini ä¸æ”¯æ´ system è§’è‰²
      .map((msg, index) => {
        console.log(`è™•ç†è¨Šæ¯ ${index + 1}:`, {
          role: msg.role,
          contentType: Array.isArray(msg.content) ? "multimodal" : "text",
          contentLength: Array.isArray(msg.content)
            ? msg.content.length
            : msg.content?.length,
        });

        const geminiMessage = {
          role: msg.role === "assistant" ? "model" : "user",
          parts: [],
        };

        // è™•ç†å¤šæ¨¡æ…‹å…§å®¹
        if (Array.isArray(msg.content)) {
          // å¦‚æœå…§å®¹æ˜¯æ•¸çµ„ï¼Œèªªæ˜åŒ…å«å¤šç¨®é¡å‹çš„å…§å®¹ï¼ˆæ–‡å­—+åœ–ç‰‡ï¼‰
          console.log("è™•ç†å¤šæ¨¡æ…‹å…§å®¹ï¼Œéƒ¨åˆ†æ•¸é‡:", msg.content.length);
          msg.content.forEach((part, partIndex) => {
            console.log(`  éƒ¨åˆ† ${partIndex + 1}:`, part.type);
            if (part.type === "text") {
              geminiMessage.parts.push({ text: part.text });
            } else if (part.type === "image") {
              geminiMessage.parts.push({
                inline_data: {
                  mime_type: part.source.media_type,
                  data: part.source.data,
                },
              });
              console.log("  æ·»åŠ åœ–ç‰‡ï¼ŒMIMEé¡å‹:", part.source.media_type);
            }
          });
        } else {
          // æ™®é€šæ–‡å­—å…§å®¹
          geminiMessage.parts.push({ text: msg.content });
        }

        console.log(
          `Geminiè¨Šæ¯ ${index + 1} éƒ¨åˆ†æ•¸é‡:`,
          geminiMessage.parts.length
        );
        return geminiMessage;
      });

    console.log("è½‰æ›å®Œæˆï¼ŒGeminiè¨Šæ¯æ•¸é‡:", result.length);
    return result;
  }

  /**
   * ä¼°ç®—æ–‡æœ¬çš„ token æ•¸é‡
   * @param {string} text - æ–‡æœ¬å…§å®¹
   * @returns {number} ä¼°ç®—çš„ token æ•¸é‡
   */
  static estimateTokens(text) {
    if (!text) return 0;
    // ç°¡å–®çš„ token ä¼°ç®—ï¼šç¹é«”ä¸­æ–‡ç´„ 1.2 token/å­—ï¼Œè‹±æ–‡ç´„ 0.75 token/word
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    const englishWords = text
      .replace(/[\u4e00-\u9fff]/g, "")
      .trim()
      .split(/\s+/)
      .filter((w) => w).length;
    return Math.ceil(chineseChars * 1.2 + englishWords * 0.75);
  }

  /**
   * è¨ˆç®— Gemini æ¨¡å‹çš„è²»ç”¨
   * @param {number} inputTokens - è¼¸å…¥ token æ•¸é‡
   * @param {number} outputTokens - è¼¸å‡º token æ•¸é‡
   * @param {string} model - æ¨¡å‹åç¨±
   * @returns {number} è²»ç”¨ï¼ˆç¾å…ƒï¼‰
   */
  static calculateGeminiCost(inputTokens, outputTokens, model) {
    // Gemini 2.0 Flash çš„å®šåƒ¹ï¼ˆ2024å¹´12æœˆï¼‰
    const pricing = {
      "gemini-2.0-flash": {
        input: 0.000075, // $0.075 per 1K tokens
        output: 0.0003, // $0.30 per 1K tokens
      },
      "gemini-1.5-pro": {
        input: 0.0035, // $3.50 per 1K tokens
        output: 0.0105, // $10.50 per 1K tokens
      },
    };

    const modelPricing = pricing[model] || pricing["gemini-2.0-flash"];
    const inputCost = (inputTokens / 1000) * modelPricing.input;
    const outputCost = (outputTokens / 1000) * modelPricing.output;

    return inputCost + outputCost;
  }

  /**
   * æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼
   * @param {string} model - æ¨¡å‹åç¨±
   * @returns {boolean} æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼
   */
  static isThinkingCapableModel(model) {
    if (!model) return false;

    const modelLower = model.toLowerCase();
    const thinkingModels = ["qwen3", "deepseek-r1", "smallthinker"];

    return thinkingModels.some((thinkingModel) =>
      modelLower.includes(thinkingModel)
    );
  }

  /**
   * è¨ˆç®—è²»ç”¨
   * @param {string} provider - æ¨¡å‹æä¾›è€…
   * @param {number} tokens_used - ä½¿ç”¨çš„ token æ•¸é‡
   * @returns {number} è²»ç”¨ï¼ˆç¾å…ƒï¼‰
   */
  static calculateCost(provider, tokens_used) {
    // å¯¦ç¾è²»ç”¨è¨ˆç®—é‚è¼¯
    // é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„è²»ç”¨è¨ˆç®—é‚è¼¯ä¾†å¯¦ç¾
    return 0; // æš«æ™‚è¿”å›0ï¼Œå¯¦éš›æ‡‰è©²æ ¹æ“šå¯¦éš›çš„è²»ç”¨è¨ˆç®—é‚è¼¯ä¾†å¯¦ç¾
  }

  /**
   * æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§
   * @param {string} provider - æ¨¡å‹æä¾›è€…
   * @param {string} model - æ¨¡å‹åç¨±
   * @returns {Promise<boolean>} æ˜¯å¦å¯ç”¨
   */
  static async checkModelAvailability(provider, model) {
    try {
      switch (provider.toLowerCase()) {
        case "ollama":
          const ollamaEndpoint =
            process.env.OLLAMA_ENDPOINT || "http://localhost:11434";
          const response = await axios.get(`${ollamaEndpoint}/api/tags`, {
            timeout: 5000,
          });
          const availableModels =
            response.data.models?.map((m) => m.name) || [];
          return availableModels.includes(model);

        case "gemini":
          const apiKey = process.env.GEMINI_API_KEY;
          if (!apiKey) return false;

          // å˜—è©¦ä¸€å€‹ç°¡å–®çš„èª¿ç”¨ä¾†æª¢æŸ¥å¯ç”¨æ€§
          await axios.post(
            `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
            {
              contents: [{ role: "user", parts: [{ text: "test" }] }],
              generationConfig: { maxOutputTokens: 1 },
            },
            { timeout: 10000 }
          );
          return true;

        default:
          return false;
      }
    } catch (error) {
      logger.debug("æ¨¡å‹å¯ç”¨æ€§æª¢æŸ¥å¤±æ•—", {
        provider,
        model,
        error: error.message,
      });
      return false;
    }
  }

  /**
   * ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
   * @returns {Promise<Object>} å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  static async getAvailableModels() {
    const models = {
      ollama: [],
      gemini: [],
    };

    try {
      // ç²å– Ollama æ¨¡å‹
      try {
        const ollamaEndpoint =
          process.env.OLLAMA_ENDPOINT || "http://localhost:11434";
        const response = await axios.get(`${ollamaEndpoint}/api/tags`, {
          timeout: 5000,
        });
        models.ollama =
          response.data.models?.map((m) => ({
            name: m.name,
            size: m.size,
            modified_at: m.modified_at,
          })) || [];
      } catch (error) {
        logger.debug("ç„¡æ³•ç²å– Ollama æ¨¡å‹åˆ—è¡¨", { error: error.message });
      }

      // ç²å– Gemini æ¨¡å‹ï¼ˆå›ºå®šåˆ—è¡¨ï¼‰
      if (process.env.GEMINI_API_KEY) {
        models.gemini = [
          {
            name: "gemini-2.0-flash",
            display_name: "Gemini 2.0 Flash",
            description: "æœ€æ–°çš„å¤šæ¨¡æ…‹æ¨¡å‹ï¼Œå¿«é€Ÿä¸”é«˜æ•ˆ",
          },
          {
            name: "gemini-1.5-pro",
            display_name: "Gemini 1.5 Pro",
            description: "å¼·å¤§çš„å¤§å‹èªè¨€æ¨¡å‹",
          },
        ];
      }

      return models;
    } catch (error) {
      logger.error("ç²å–å¯ç”¨æ¨¡å‹å¤±æ•—", { error: error.message });
      return models;
    }
  }
}

export default AIService;
